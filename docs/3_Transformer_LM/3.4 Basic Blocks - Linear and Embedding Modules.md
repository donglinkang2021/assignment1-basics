## 3.4 Basic Building Blocks: Linear and Embedding Modules

### 3.4.1 Parameter Initialization

Training neural networks effectively often requires careful initialization of the model parameters—bad initializations can lead to undesirable behavior such as vanishing or exploding gradients. Pre-norm transformers are unusually robust to initializations, but they can still have a siginificant impact on training speed and convergence. Since this assignment is already long, we will save the details for assignment 3, and instead give you some approximate initializations that should work well for most cases. For now, use:

-   **Linear weights**: $N(\mu=0, \sigma^2 = \frac{2}{d_{in}+d_{out}})$ truncated at $[-3\sigma, 3\sigma]$.
-   **Embedding**: $N(\mu=0, \sigma^2=1)$ truncated at $[-3, 3]$.
-   **RMSNorm**: 1

You should use `torch.nn.init.trunc_normal_` to initialize the truncated normal weights.

### 3.4.2 Linear Module

Linear layers are a fundamental building block of Transformers and neural nets in general. First, you will implement your own `Linear` class that inherits from `torch.nn.Module` and performs a linear transformation: $y = Wx$. Note that we do not include a bias term, following most modern LLMs.

> **Problem (linear): Implementing the linear module (1 point)**
>
> **Deliverable**: Implement a `Linear` class that inherits from `torch.nn.Module` and performs a linear transformation. Your implementation should follow the interface of PyTorch’s built-in `nn.Linear` module, except for not having a bias argument or parameter. We recommend the following interface:
>
> `def __init__(self, in_features, out_features, device=None, dtype=None)`: Construct a linear transformation module. This function should accept the following parameters:
>
> -   `in_features: int`: final dimension of the input
> -   `out_features: int`: final dimension of the output
> -   `device: torch.device | None = None`: Device to store the parameters on
> -   `dtype: torch.dtype | None = None`: Data type of the parameters
>
> `def forward(self, x: torch.Tensor) -> torch.Tensor`: Apply the linear transformation to the input.
>
> Make sure to:
>
> -   subclass `nn.Module`
> -   call the superclass constructor
> -   construct and store your parameter as `W` (not `W^T`) for memory ordering reasons, putting it in an `nn.Parameter`
> -   of course, don’t use `nn.Linear` or `nn.functional.linear`
>
> For initializations, use the settings from above along with `torch.nn.init.trunc_normal_` to initialize the weights.
>
> To test your `Linear` module, implement the test adapter at `[adapters.run_linear]`. The adapter should load the given weights into your `Linear` module. You can use `Module.load_state_dict` for this purpose. Then, run `uv run pytest -k test_linear`.

### 3.4.3 Embedding Module

As discussed above, the first layer of the Transformer is an embedding layer that maps integer token IDs into a vector space of dimension `d_model`. We will implement a custom `Embedding` class that inherits from `torch.nn.Module` (so you should not use `nn.Embedding`). The `forward` method should select the embedding vector for each token ID by indexing into an embedding matrix of shape `(vocab_size, d_model)` using a `torch.LongTensor` of token IDs with shape `(batch_size, sequence_length)`.

> **Problem (embedding): Implement the embedding module (1 point)**
>
> **Deliverable**: Implement the `Embedding` class that inherits from `torch.nn.Module` and performs an embedding lookup. Your implementation should follow the interface of PyTorch’s built-in `nn.Embedding` module. We recommend the following interface:
>
> `def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None)`: Construct an embedding module. This function should accept the following parameters:
>
> -   `num_embeddings: int`: Size of the vocabulary
> -   `embedding_dim: int`: Dimension of the embedding vectors, i.e., `d_model`
> -   `device: torch.device | None = None`: Device to store the parameters on
> -   `dtype: torch.dtype | None = None`: Data type of the parameters
>
> `def forward(self, token_ids: torch.Tensor) -> torch.Tensor`: Lookup the embedding vectors for the given token IDs.
>
> Make sure to:
>
> -   subclass `nn.Module`
> -   call the superclass constructor
> -   initialize your embedding matrix as a `nn.Parameter`
> -   store the embedding matrix with the `d_model` being the final dimension
> -   of course, don’t use `nn.Embedding` or `nn.functional.embedding`
>
> Again, use the settings from above for initialization, and use `torch.nn.init.trunc_normal_` to initialize the weights.
>
> To test your implementation, implement the test adapter at `[adapters.run_embedding]`. Then, run `uv run pytest -k test_embedding`.
