## 3.6 The Full Transformer LM

Let’s begin by assembling the Transformer block (it will be helpful to refer back to Figure 2). A Transformer block contains two ‘sublayers’, one for the multihead self attention, and another for the feed-forward network. In each sublayer, we first perform RMSNorm, then the main operation (MHA/FF), finally adding in the residual connection.

To be concrete, the first half (the first ‘sub-layer’) of the Transformer block should be implementing the following set of updates to produce an output `y` from an input `x`:

> `y = x + MultiHeadSelfAttention(RMSNorm(x))` (15)

**Problem (transformer_block):** Implement the Transformer block (3 points)

Implement the pre-norm Transformer block as described in §3.5 and illustrated in Figure 2. Your Transformer block should accept (at least) the following parameters.

-   `d_model`: `int` Dimensionality of the Transformer block inputs.
-   `num_heads`: `int` Number of heads to use in multi-head self-attention.
-   `d_ff`: `int` Dimensionality of the position-wise feed-forward inner layer.

To test your implementation, implement the adapter `[adapters.run_transformer_block]`. Then run `uv run pytest -k test_transformer_block` to test your implementation.

**Deliverable:** Transformer block code that passes the provided tests.

---

Now we put the blocks together, following the high-level diagram in Figure 1. Follow our description of the embedding in Section 3.1.1, feed this into `num_layers` Transformer blocks, and then pass that into the three output layers to obtain a distribution over the vocabulary.

**Problem (transformer_lm):** Implementing the Transformer LM (3 points)

Time to put it all together! Implement the Transformer language model as described in §3.1 and illustrated in Figure 1. At minimum, your implementation should accept all the aforementioned construction parameters for the Transformer block, as well as these additional parameters:

-   `vocab_size`: `int` The size of the vocabulary, necessary for determining the dimensionality of the token embedding matrix.
-   `context_length`: `int` The maximum context length, necessary for determining the dimensionality of the position embedding matrix.
-   `num_layers`: `int` The number of Transformer blocks to use.

To test your implementation against our provided tests, you will first need to implement the test adapter at `[adapters.run_transformer_lm]`. Then, run `uv run pytest -k test_transformer_lm` to test your implementation.

**Deliverable:** A Transformer LM module that passes the above tests.

---

**Resource accounting.** It is useful to be able to understand how the various parts of the Transformer consume compute and memory. We will go through the steps to do some basic “FLOPs accounting.” The vast majority of FLOPS in a Transformer are matrix multiplies, so our core approach is simple:

1.  Write down all the matrix multiplies in a Transformer forward pass.
2.  Convert each matrix multiply into FLOPs required.

For this second step, the following fact will be useful:

> **Rule:** Given $A \in R^{m\times n}$ and $B \in R^{n\times p}$, the matrix-matrix product $AB$ requires $2mnp$ FLOPs.

To see this, note that $(AB)[i,j] = A[i,:] · B[:,j]$, and that this dot product requires $n$ additions and $n$ multiplications ($2n$ FLOPs). Then, since the matrix-matrix product $AB$ has $m\times p$ entries, the total number of FLOPS is $(2n)(mp) = 2mnp$.

Now, before you do the next problem, it can be helpful to go through each component of your Transformer block and Transformer LM, and list out all the matrix multiplies and their associated FLOPs costs.

**Problem (transformer_accounting):** Transformer LM resource accounting (5 points)

**(a)** Consider GPT-2 XL, which has the following configuration:
-   `vocab_size`: 50,257
-   `context_length`: 1,024
-   `num_layers`: 48
-   `d_model`: 1,600
-   `num_heads`: 25
-   `d_ff`: 6,400

Suppose we constructed our model using this configuration. How many trainable parameters would our model have? Assuming each parameter is represented using single-precision floating point, how much memory is required to just load this model?

**Deliverable:** A one-to-two sentence response.

**(b)** Identify the matrix multiplies required to complete a forward pass of our GPT-2-XL-shaped model. How many FLOPs do these matrix multiplies require in total? Assume that our input sequence has `context_length` tokens.

**Deliverable:** A list of matrix multiplies (with descriptions), and the total number of FLOPs required.

**(c)** Based on your analysis above, which parts of the model require the most FLOPs?

**Deliverable:** A one-to-two sentence response.

**(d)** Repeat your analysis with GPT-2 small (12 layers, 768 `d_model`, 12 heads), GPT-2 medium (24 layers, 1024 `d_model`, 16 heads), and GPT-2 large (36 layers, 1280 `d_model`, 20 heads). As the model size increases, which parts of the Transformer LM take up proportionally more or less of the total FLOPs?

**Deliverable:** For each model, provide a breakdown of model components and its associated FLOPs (as a proportion of the total FLOPs required for a forward pass). In addition, provide a one-to-two sentence description of how varying the model size changes the proportional FLOPs of each component.

**(e)** Take GPT-2 XL and increase the context length to 16,384. How does the total FLOPs for one forward pass change? How do the relative contribution of FLOPs of the model components change?

**Deliverable:** A one-to-two sentence response.