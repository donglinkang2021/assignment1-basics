## 4.1 Cross-entropy loss

Recall that the Transformer language model defines a distribution $p_\theta(x_{i+1} | x_{1:i})$ for each sequence $\mathbf{x}$ of length $m+1$ and $i = 1, \dots, m$. Given a training set $D$ consisting of sequences of length $m$, we define the standard cross-entropy (negative log-likelihood) loss function:

$$
\ell(\theta; D) = \frac{1}{|D|m} \sum_{\mathbf{x} \in D} \sum_{i=1}^{m} -\log p_\theta(x_{i+1} | x_{1:i}).
$$

(Note that a single forward pass in the Transformer yields $p_\theta(x_{i+1} | x_{1:i})$ for all $i = 1, \dots, m$.)

In particular, the Transformer computes logits $o_i \in \mathbb{R}^{\text{vocab\_size}}$ for each position $i$, which results in:[^6]

$$
p(x_{i+1} | x_{1:i}) = \text{softmax}(o_i)[x_{i+1}] = \frac{\exp(o_i[x_{i+1}])}{\sum_{a=1}^{\text{vocab\_size}} \exp(o_i[a])}.
$$

The cross entropy loss is generally defined with respect to the vector of logits $o_i \in \mathbb{R}^{\text{vocab\_size}}$ and target $x_{i+1}$.[^7]

Implementing the cross entropy loss requires some care with numerical issues, just like in the case of softmax.

**Problem (cross\_entropy): Implement Cross entropy**

**Deliverable:** Write a function to compute the cross entropy loss, which takes in predicted logits ($o_i$) and targets ($x_{i+1}$) and computes the cross entropy $\ell_i = -\log \text{softmax}(o_i)[x_{i+1}]$. Your function should handle the following:

- Subtract the largest element for numerical stability.
- Cancel out `log` and `exp` whenever possible.
- Handle any additional batch dimensions and return the average across the batch. As with section 3.3, we assume batch-like dimensions always come first, before the vocabulary size dimension.

Implement `[adapters.run_cross_entropy]`, then run `uv run pytest -k test_cross_entropy` to test your implementation.

### Perplexity

Cross entropy suffices for training, but when we evaluate the model, we also want to report perplexity. For a sequence of length $m$ where we suffer cross-entropy losses $\ell_1, \dots, \ell_m$:

$$
\text{perplexity} = \exp\left(\frac{1}{m} \sum_{i=1}^{m} \ell_i\right).
$$

[^6]: Note that $o_i[k]$ refers to value at index $k$ of the vector $o_i$.
[^7]: This corresponds to the cross entropy between the Dirac delta distribution over $x_{i+1}$ and the predicted $\text{softmax}(o_i)$ distribution.
