## 4.4 Learning rate scheduling

The value for the learning rate that leads to the quickest decrease in loss often varies during training. In training Transformers, it is typical to use a learning rate schedule, where we start with a bigger learning rate, making quicker updates in the beginning, and slowly decay it to a smaller value as the model trains.[^8]

In this assignment, we will implement the cosine annealing schedule used to train LLaMA [Touvron et al., 2023].

A scheduler is simply a function that takes the current step $t$ and other relevant parameters (such as the initial and final learning rates), and returns the learning rate to use for the gradient update at step $t$. The simplest schedule is the constant function, which will return the same learning rate given any $t$.

The cosine annealing learning rate schedule takes (i) the current iteration $t$, (ii) the maximum learning rate $\alpha_{\max}$, (iii) the minimum (final) learning rate $\alpha_{\min}$, (iv) the number of warm-up iterations $T_w$, and (v) the number of cosine annealing iterations $T_c$. The learning rate at iteration $t$ is defined as:

-   **(Warm-up)** If $t < T_w$, then $\alpha_t = \frac{t}{T_w} \alpha_{\max}$.
-   **(Cosine annealing)** If $T_w \le t \le T_c$, then $\alpha_t = \alpha_{\min} + \frac{1}{2} \left(1 + \cos\left(\frac{t - T_w}{T_c - T_w} \pi\right)\right) (\alpha_{\max} - \alpha_{\min})$.
-   **(Post-annealing)** If $t > T_c$, then $\alpha_t = \alpha_{\min}$.

[^8]: Itâ€™s sometimes common to use a schedule where the learning rate rises back up (restarts) to help get past local minima.

**Problem (learning_rate_schedule): Implement cosine learning rate schedule with warmup**

Write a function that takes $t$, $\alpha_{\max}$, $\alpha_{\min}$, $T_w$ and $T_c$, and returns the learning rate $\alpha_t$ according to the scheduler defined above. Then implement `[adapters.get_lr_cosine_schedule]` and make sure it passes `uv run pytest -k test_get_lr_cosine_schedule`.
