## 4.3 AdamW

Modern language models are typically trained with more sophisticated optimizers, instead of SGD. Most optimizers used recently are derivatives of the Adam optimizer [Kingma and Ba, 2015]. We will use AdamW [Loshchilov and Hutter, 2019], which is in wide use in recent work. AdamW proposes a modification to Adam that improves regularization by adding weight decay (at each iteration, we pull the parameters towards 0), in a way that is decoupled from the gradient update. We will implement AdamW as described in algorithm 2 of Loshchilov and Hutter [2019].

AdamW is stateful: for each parameter, it keeps track of a running estimate of its first and second moments. Thus, AdamW uses additional memory in exchange for improved stability and convergence. Besides the learning rate $\alpha$, AdamW has a pair of hyperparameters $(\beta_1, \beta_2)$ that control the updates to the moment estimates, and a weight decay rate $\lambda$. Typical applications set $(\beta_1, \beta_2)$ to $(0.9, 0.999)$, but large language models like LLaMA [Touvron et al., 2023] and GPT-3 [Brown et al., 2020] are often trained with $(0.9, 0.95)$. The algorithm can be written as follows, where $\epsilon$ is a small value (e.g., $10^{-8}$) used to improve numerical stability in case we get extremely small values in $v$:

**Algorithm 1: AdamW Optimizer**

-   `init(θ)` (Initialize learnable parameters)
-   $m \leftarrow 0$ (Initial value of the first moment vector; same shape as $\theta$)
-   $v \leftarrow 0$ (Initial value of the second moment vector; same shape as $\theta$)
-   **for** $t = 1, \dots, T$ **do**:
    -   Sample batch of data $B_t$
    -   $g \leftarrow \nabla_{\theta} \ell(\theta; B_t)$ (Compute the gradient of the loss at the current time step)
    -   $m \leftarrow \beta_1 m + (1 - \beta_1) g$ (Update the first moment estimate)
    -   $v \leftarrow \beta_2 v + (1 - \beta_2) g^2$ (Update the second moment estimate)
    -   $\hat{\alpha}_t \leftarrow \alpha \frac{\sqrt{1 - \beta_2^t}}{1 - \beta_1^t}$ (Compute bias-corrected learning rate for iteration $t$)
    -   $\theta \leftarrow \theta - \hat{\alpha}_t \frac{m}{\sqrt{v} + \epsilon}$ (Update the parameters)
    -   $\theta \leftarrow \theta - \alpha \lambda \theta$ (Apply weight decay)
-   **end for**

Note that $t$ starts at 1. You will now implement this optimizer.

**Problem (adamw): Implement AdamW (2 points)**

**Deliverable:** Implement the AdamW optimizer as a subclass of `torch.optim.Optimizer`. Your class should take the learning rate $\alpha$ in `__init__`, as well as the $\beta$, $\epsilon$ and $\lambda$ hyperparameters. To help you keep state, the base `Optimizer` class gives you a dictionary `self.state`, which maps `nn.Parameter` objects to a dictionary that stores any information you need for that parameter (for AdamW, this would be the moment estimates). Implement `[adapters.get_adamw_cls]` and make sure it passes `uv run pytest -k test_adamw`.

**Problem (adamwAccounting): Resource accounting for training with AdamW (2 points)**

Let us compute how much memory and compute running AdamW requires. Assume we are using `float32` for every tensor.

(a) How much peak memory does running AdamW require? Decompose your answer based on the memory usage of the parameters, activations, gradients, and optimizer state. Express your answer in terms of the `batch_size` and the model hyperparameters (`vocab_size`, `context_length`, `num_layers`, `d_model`, `num_heads`). Assume `d_ff = 4 * d_model`.

For simplicity, when calculating memory usage of activations, consider only the following components:

-   Transformer block
    -   RMSNorm(s)
    -   Multi-head self-attention sublayer: QKV projections, $Q^T K$ matrix multiply, softmax, weighted sum of values, output projection.
    -   Position-wise feed-forward: $W_1$ matrix multiply, SiLU, $W_2$ matrix multiply
-   final RMSNorm
-   output embedding
-   cross-entropy on logits

**Deliverable:** An algebraic expression for each of parameters, activations, gradients, and optimizer state, as well as the total.

(b) Instantiate your answer for a GPT-2-XL-shaped model to get an expression that only depends on the `batch_size`. What is the maximum batch size you can use and still fit within 80GB memory?

**Deliverable:** An expression that looks like $a \cdot \text{batch\_size} + b$ for numerical values $a, b$, and a number representing the maximum batch size.

(c) How many FLOPs does running one step of AdamW take?

**Deliverable:** An algebraic expression, with a brief justification.

(d) Model FLOPs utilization (MFU) is defined as the ratio of observed throughput (tokens per second) relative to the hardware’s theoretical peak FLOP throughput [Chowdhery et al., 2022]. An NVIDIA A100 GPU has a theoretical peak of 19.5 teraFLOP/s for `float32` operations. Assuming you are able to get 50% MFU, how long would it take to train a GPT-2-XL for 400K steps and a batch size of 1024 on a single A100? Following Kaplan et al. [2020] and Hoffmann et al. [2022], assume that the backward pass has twice the FLOPs of the forward pass.

**Deliverable:** The number of days training would take, with a brief justification.
